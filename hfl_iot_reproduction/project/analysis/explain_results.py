from __future__ import annotations

import re
from datetime import datetime, timezone
from pathlib import Path

import pandas as pd

from project.analysis.paths import OUT

CSV = OUT / "metrics_all.csv"
REPORT = OUT / "results_explanation.md"


def _fmt_float(value, ndigits: int = 4) -> str:
    if value is None or pd.isna(value):
        return "n/a"
    return f"{float(value):.{ndigits}f}"


def _fmt_int(value) -> str:
    if value is None or pd.isna(value):
        return "n/a"
    return str(int(value))


def _fmt_pct(value, ndigits: int = 1) -> str:
    if value is None or pd.isna(value):
        return "n/a"
    return f"{100.0 * float(value):.{ndigits}f}%"


def _fmt_ts(ms) -> str:
    if ms is None or pd.isna(ms):
        return "n/a"
    return datetime.fromtimestamp(float(ms) / 1000.0, tz=timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")


def _duration_seconds(df: pd.DataFrame) -> float | None:
    if df.empty or "_ts" not in df.columns:
        return None
    ts = df["_ts"].dropna()
    if len(ts) < 2:
        return None
    return float((ts.max() - ts.min()) / 1000.0)


def _md_table(headers: list[str], rows: list[list[str]]) -> str:
    if not rows:
        return "_No data available._"
    out = [
        "| " + " | ".join(headers) + " |",
        "| " + " | ".join(["---"] * len(headers)) + " |",
    ]
    for row in rows:
        out.append("| " + " | ".join(row) + " |")
    return "\n".join(out)


def _artifact_description(name: str) -> str:
    if name == "README.md":
        return "Short folder guide for generated analysis artifacts."
    if name == "metrics_all.csv":
        return "Unified raw events/metrics generated by extract_metrics.py from logs/*.jsonl."
    if name == "iot_summary.csv":
        return "Per-IoT summary table generated by summarize.py."
    if name == "edge_summary.csv":
        return "Per-edge summary table generated by summarize.py."
    if name == "cloud_summary.csv":
        return "Cloud-level summary table generated by summarize.py."
    if name == "results_explanation.md":
        return "Narrative explanation generated by explain_results.py/summarize.py."
    if name.startswith("iot_") and name.endswith("_val_acc.png"):
        return "IoT validation accuracy per local round (from plot_curves.py)."
    if name.startswith("iot_") and name.endswith("_val_loss.png"):
        return "IoT validation loss per local round (from plot_curves.py)."
    if name.startswith("edge_") and name.endswith("_win_p.png"):
        return "Edge sliding-window size and participation (pactual) over time."
    if name == "cloud_edges.png":
        return "Cloud round index versus number of contributing edges."
    if name.endswith(".png"):
        return "Plot generated by plot_curves.py."
    if name.endswith(".csv"):
        return "CSV artifact generated by analysis scripts."
    return "Output artifact."


def _collect_artifacts() -> list[tuple[str, int, str]]:
    rows = []
    for path in sorted(OUT.glob("*")):
        if path.is_file():
            rows.append((path.name, int(path.stat().st_size), _artifact_description(path.name)))
    return rows


def _build_iot_summary(iot_df: pd.DataFrame):
    rows = []
    by_iot = {}
    if iot_df.empty:
        return rows, by_iot

    for iot_id, sub in sorted(iot_df.groupby("iot"), key=lambda kv: str(kv[0])):
        sub = sub.sort_values("round")
        rounds = int(sub["round"].max()) if "round" in sub.columns and sub["round"].notna().any() else len(sub)

        acc = sub["val_acc"].dropna() if "val_acc" in sub.columns else pd.Series(dtype=float)
        loss = sub["val_loss"].dropna() if "val_loss" in sub.columns else pd.Series(dtype=float)

        first_acc = float(acc.iloc[0]) if len(acc) > 0 else None
        last_acc = float(acc.iloc[-1]) if len(acc) > 0 else None
        best_acc = float(acc.max()) if len(acc) > 0 else None
        last_loss = float(loss.iloc[-1]) if len(loss) > 0 else None
        if len(acc) >= 10:
            trend = float(acc.tail(5).mean() - acc.head(5).mean())
        elif len(acc) >= 2:
            trend = float(acc.iloc[-1] - acc.iloc[0])
        else:
            trend = None

        by_iot[str(iot_id)] = {
            "rounds": rounds,
            "first_acc": first_acc,
            "last_acc": last_acc,
            "best_acc": best_acc,
            "last_loss": last_loss,
            "trend": trend,
        }

        rows.append(
            [
                str(iot_id),
                _fmt_int(rounds),
                _fmt_float(first_acc),
                _fmt_float(last_acc),
                _fmt_float(best_acc),
                _fmt_float(last_loss),
                _fmt_float(trend),
            ]
        )

    return rows, by_iot


def _build_edge_summary(edge_df: pd.DataFrame):
    rows = []
    by_edge = {}
    if edge_df.empty:
        return rows, by_edge

    for edge_id, sub in sorted(edge_df.groupby("edge"), key=lambda kv: str(kv[0])):
        sub = sub.sort_values("_ts")
        win = sub["window"].dropna() if "window" in sub.columns else pd.Series(dtype=float)
        p = sub["pactual"].dropna() if "pactual" in sub.columns else pd.Series(dtype=float)
        q = sub["qcurrent"].dropna() if "qcurrent" in sub.columns else pd.Series(dtype=float)
        b = sub["buf"].dropna() if "buf" in sub.columns else pd.Series(dtype=float)

        window_start = float(win.iloc[0]) if len(win) > 0 else None
        window_last = float(win.iloc[-1]) if len(win) > 0 else None
        p_half_ratio = float((p == 0.5).mean()) if len(p) > 0 else None
        p_full_ratio = float((p == 1.0).mean()) if len(p) > 0 else None
        q_mean = float(q.mean()) if len(q) > 0 else None
        buf_mean = float(b.mean()) if len(b) > 0 else None

        by_edge[str(edge_id)] = {
            "window_start": window_start,
            "window_last": window_last,
            "window_min": float(win.min()) if len(win) > 0 else None,
            "window_max": float(win.max()) if len(win) > 0 else None,
            "p_half_ratio": p_half_ratio,
            "p_full_ratio": p_full_ratio,
            "q_mean": q_mean,
            "buf_mean": buf_mean,
        }

        rows.append(
            [
                str(edge_id),
                _fmt_float(window_start, 1),
                _fmt_float(window_last, 1),
                _fmt_float(by_edge[str(edge_id)]["window_min"], 1),
                _fmt_float(by_edge[str(edge_id)]["window_max"], 1),
                _fmt_pct(p_half_ratio),
                _fmt_pct(p_full_ratio),
                _fmt_float(q_mean),
                _fmt_float(buf_mean, 2),
            ]
        )

    return rows, by_edge


def _build_cloud_summary(cloud_df: pd.DataFrame):
    if cloud_df.empty:
        return None

    sub = cloud_df.copy()
    if "round" in sub.columns:
        sub = sub.sort_values("round")
    else:
        sub = sub.sort_values("_ts")

    edges = sub["edges"].dropna() if "edges" in sub.columns else pd.Series(dtype=float)
    beta = sub["beta_cloud"].dropna() if "beta_cloud" in sub.columns else pd.Series(dtype=float)

    duration_s = _duration_seconds(sub)
    updates_per_min = (len(sub) / (duration_s / 60.0)) if duration_s and duration_s > 0 else None

    return {
        "rows": len(sub),
        "round_min": int(sub["round"].min()) if "round" in sub.columns and sub["round"].notna().any() else None,
        "round_max": int(sub["round"].max()) if "round" in sub.columns and sub["round"].notna().any() else None,
        "edges_min": float(edges.min()) if len(edges) > 0 else None,
        "edges_mean": float(edges.mean()) if len(edges) > 0 else None,
        "edges_max": float(edges.max()) if len(edges) > 0 else None,
        "beta_mode": float(beta.mode().iloc[0]) if len(beta) > 0 else None,
        "updates_per_min": updates_per_min,
        "duration_s": duration_s,
    }


def _plot_interpretation_lines(iot_map: dict, edge_map: dict, cloud_summary: dict | None, artifact_names: list[str]) -> list[str]:
    lines = []
    for name in artifact_names:
        m = re.match(r"^iot_(.+)_val_acc\.png$", name)
        if m:
            iot_id = m.group(1)
            info = iot_map.get(iot_id, {})
            lines.append(
                f"- `{name}`: evolution of IoT `{iot_id}` validation accuracy across local rounds; "
                f"last={_fmt_float(info.get('last_acc'))}, best={_fmt_float(info.get('best_acc'))}."
            )
            continue

        m = re.match(r"^iot_(.+)_val_loss\.png$", name)
        if m:
            iot_id = m.group(1)
            info = iot_map.get(iot_id, {})
            lines.append(
                f"- `{name}`: evolution of IoT `{iot_id}` validation loss; "
                f"last={_fmt_float(info.get('last_loss'))}."
            )
            continue

        m = re.match(r"^edge_(.+)_win_p\.png$", name)
        if m:
            edge_id = m.group(1)
            info = edge_map.get(edge_id, {})
            lines.append(
                f"- `{name}`: edge `{edge_id}` control behavior (window and participation); "
                f"window { _fmt_float(info.get('window_start'), 1) } -> { _fmt_float(info.get('window_last'), 1) }, "
                f"pactual=0.5 in {_fmt_pct(info.get('p_half_ratio'))} of observations."
            )
            continue

        if name == "cloud_edges.png":
            if cloud_summary:
                lines.append(
                    f"- `{name}`: number of edges seen by cloud per aggregation round; "
                    f"min/mean/max={_fmt_float(cloud_summary.get('edges_min'), 1)}/"
                    f"{_fmt_float(cloud_summary.get('edges_mean'), 2)}/"
                    f"{_fmt_float(cloud_summary.get('edges_max'), 1)}."
                )
            else:
                lines.append(f"- `{name}`: number of edges seen by cloud per aggregation round.")
            continue

    return lines


def generate_results_explanation(df: pd.DataFrame | None = None) -> Path:
    if df is None:
        if not CSV.exists():
            raise FileNotFoundError(f"Missing {CSV}. Run extract_metrics first.")
        df = pd.read_csv(CSV)

    metrics = df[df["type"] == "metric"].copy() if "type" in df.columns else pd.DataFrame()
    events = df[df["type"] == "event"].copy() if "type" in df.columns else pd.DataFrame()

    iot = metrics[metrics["file"].str.startswith("iot")] if not metrics.empty and "file" in metrics.columns else pd.DataFrame()
    edge = metrics[metrics["file"].str.startswith("edge")] if not metrics.empty and "file" in metrics.columns else pd.DataFrame()
    cloud = metrics[metrics["file"].str.startswith("cloud")] if not metrics.empty and "file" in metrics.columns else pd.DataFrame()

    iot_rows, iot_map = _build_iot_summary(iot)
    edge_rows, edge_map = _build_edge_summary(edge)
    cloud_summary = _build_cloud_summary(cloud)

    iot_rounds = [v["rounds"] for v in iot_map.values() if v.get("rounds") is not None]
    min_iot_round = min(iot_rounds) if iot_rounds else None
    max_iot_round = max(iot_rounds) if iot_rounds else None
    iot_round_gap = (max_iot_round - min_iot_round) if iot_rounds else None

    artifact_rows = _collect_artifacts()
    artifact_names = [row[0] for row in artifact_rows]
    plot_lines = _plot_interpretation_lines(iot_map, edge_map, cloud_summary, artifact_names)

    run_duration_s = _duration_seconds(metrics)
    generated_at = datetime.now(tz=timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")

    report_lines = [
        "# Results Explanation (IoT -> Edge -> Cloud)",
        "",
        f"Generated at: {generated_at}",
        "",
        "## App Context Used For Interpretation",
        "- IoT behavior comes from `project/iot/device.py`: each device trains locally, reports `val_acc`/`val_loss`, encrypts weights, and sends updates to its edge.",
        "- Edge behavior comes from `project/edge/server.py`: asynchronous buffering plus aggregation `w_edge = (alpha*w_edge + beta*sum(w_i)) / (alpha + beta*|A|)` and adaptive window update `window = max(3, int(window + alpha_sw*(pdesired - pactual)))`.",
        "- Cloud behavior comes from `project/cloud/server.py`: weighted edge fusion using `v_e = beta_cloud*p_e + (1-beta_cloud)*q_e`, then global model broadcast to all connected edges.",
        "- Analysis pipeline context comes from `project/analysis/extract_metrics.py`, `project/analysis/plot_curves.py`, and `project/analysis/summarize.py`.",
        "",
        "## Input Snapshot",
        f"- Source CSV: `{CSV.name}` with {len(df)} rows and columns: {', '.join(df.columns.tolist())}.",
        f"- Metric rows: {len(metrics)}; event rows: {len(events)}.",
        f"- Metric time span: {_fmt_ts(metrics['_ts'].min() if not metrics.empty else None)} to {_fmt_ts(metrics['_ts'].max() if not metrics.empty else None)}.",
        f"- Metric duration (approx): {_fmt_float(run_duration_s, 2)} seconds.",
        "",
        "## Output Artifacts",
        _md_table(
            ["Artifact", "Size (bytes)", "Meaning"],
            [[name, str(size), meaning] for (name, size, meaning) in artifact_rows],
        ),
        "",
        "## IoT Layer Results",
        _md_table(
            ["IoT", "Rounds", "First Acc", "Last Acc", "Best Acc", "Last Loss", "Acc Trend"],
            iot_rows,
        ),
        "",
        f"- Fastest IoT completed {_fmt_int(max_iot_round)} rounds; slowest completed {_fmt_int(min_iot_round)} rounds; gap={_fmt_int(iot_round_gap)} rounds.",
        "- This gap is expected in this app because updates are asynchronous and each IoT proceeds independently (no global per-round barrier).",
        "",
        "## Edge Layer Results",
        _md_table(
            [
                "Edge",
                "Window Start",
                "Window Last",
                "Window Min",
                "Window Max",
                "pactual=0.5",
                "pactual=1.0",
                "qcurrent Mean",
                "Buffer Mean",
            ],
            edge_rows,
        ),
        "",
        "- In this run, both edges reduced their effective window from 6 to 3 in logs, matching the adaptive policy that shrinks window when participation exceeds the desired level.",
        "- Frequent `pactual=0.5` indicates many edge aggregation steps were triggered by one of the two connected IoTs per edge rather than both arriving together.",
        "",
        "## Cloud Layer Results",
    ]

    if cloud_summary is None:
        report_lines.extend(
            [
                "- No cloud metrics were found in `metrics_all.csv`.",
                "",
            ]
        )
    else:
        report_lines.extend(
            [
                f"- Cloud metric rows: {_fmt_int(cloud_summary.get('rows'))}.",
                f"- Cloud rounds observed: {_fmt_int(cloud_summary.get('round_min'))} to {_fmt_int(cloud_summary.get('round_max'))}.",
                f"- Contributing edges (min/mean/max): {_fmt_float(cloud_summary.get('edges_min'), 1)}/"
                f"{_fmt_float(cloud_summary.get('edges_mean'), 2)}/"
                f"{_fmt_float(cloud_summary.get('edges_max'), 1)}.",
                f"- Observed `beta_cloud` mode: {_fmt_float(cloud_summary.get('beta_mode'), 3)}.",
                f"- Cloud aggregation frequency (approx): {_fmt_float(cloud_summary.get('updates_per_min'), 2)} updates/min.",
                "",
                "- Cloud rounds can exceed individual IoT local rounds in this implementation because cloud increments on each received edge aggregation, not on synchronized global barriers.",
                "",
            ]
        )

    report_lines.extend(
        [
            "## Figure-by-Figure Reading Guide",
        ]
    )
    if plot_lines:
        report_lines.extend(plot_lines)
    else:
        report_lines.append("- No PNG plots found in outputs.")

    report_lines.extend(
        [
            "",
            "## Important Interpretation Notes",
            "- These outputs expose local validation metrics (IoT), edge participation/quality proxies, and cloud connectivity rounds.",
            "- There is no explicit global test-accuracy metric logged by cloud in current app code, so model quality at global scope must be inferred indirectly from IoT validation trajectories.",
            "- Multiple `start` events in logs typically indicate services restarted and appended to the same log files; this can inflate total event counts while metric trends remain usable.",
        ]
    )

    OUT.mkdir(parents=True, exist_ok=True)
    tmp = REPORT.with_suffix(REPORT.suffix + ".tmp")
    tmp.write_text("\n".join(report_lines) + "\n", encoding="utf-8")
    tmp.replace(REPORT)
    return REPORT


def main():
    if not CSV.exists():
        print("Run extract_metrics first.")
        return
    df = pd.read_csv(CSV)
    out = generate_results_explanation(df)
    print(f"[analysis] wrote {out}")


if __name__ == "__main__":
    main()

